{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import sys\n",
    "import os\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "utils_dir = os.path.abspath(os.path.join(current_dir, '../'))\n",
    "\n",
    "sys.path.append(utils_dir)\n",
    "from utils.utils import NORM_COLUMNS, start_date_list, end_date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_pd = pd.read_csv(\"../utils/117_ids.csv\")\n",
    "# ids = ids_pd['nhdhr_id'].to_list()\n",
    "\n",
    "# do_raw_path = \"../data/raw/lakes/\"\n",
    "# temp_raw_path = '../data/raw/lakes_temp/'\n",
    "# save_path = '../data/processedByLake/'\n",
    "# os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# mean_feats = {feature: [] for feature in NORM_COLUMNS}\n",
    "# std_feats = {feature: [] for feature in NORM_COLUMNS}\n",
    "\n",
    "# for lake_id in ids:\n",
    "#     lake_data_path = os.path.join(do_raw_path, f\"merged_{lake_id}.csv\")\n",
    "    \n",
    "#     \n",
    "#     try:\n",
    "#         use_do_data = pd.read_csv(lake_data_path, usecols=NORM_COLUMNS)\n",
    "        \n",
    "#         \n",
    "#         mean_by_lake = use_do_data.mean()\n",
    "#         std_by_lake = use_do_data.std()\n",
    "        \n",
    "#         \n",
    "#         for feature in NORM_COLUMNS:\n",
    "#             mean_feats[feature].append(mean_by_lake[feature])\n",
    "#             std_feats[feature].append(std_by_lake[feature])\n",
    "        \n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File for lake {lake_id} not found.\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "# average_mean_feats = {feature: np.mean(mean_feats[feature]) for feature in NORM_COLUMNS}\n",
    "# average_std_feats = {feature: np.mean(std_feats[feature]) for feature in NORM_COLUMNS}\n",
    "\n",
    "# os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# np.save(os.path.join(save_path, 'mean_feats.npy'), average_mean_feats)\n",
    "# np.save(os.path.join(save_path, 'std_feats.npy'), average_std_feats)\n",
    "\n",
    "# loaded_mean_feats = np.load(os.path.join(save_path, 'mean_feats.npy'), allow_pickle=True).item()\n",
    "# loaded_std_feats = np.load(os.path.join(save_path, 'std_feats.npy'), allow_pickle=True).item()\n",
    "\n",
    "# print(\"Loaded Mean Features:\", loaded_mean_feats)\n",
    "# print(\"Loaded Std Features:\", loaded_std_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_pd = pd.read_csv(\"../utils/117_ids.csv\")\n",
    "ids = ids_pd['nhdhr_id'].to_list()\n",
    "\n",
    "do_raw_path = \"../data/raw/lakes/\"\n",
    "temp_raw_path = '../data/raw/lakes_temp/'\n",
    "save_path = '../data/processedByLake/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for lake_id in tqdm(ids):\n",
    "\n",
    "    lake_data_path = os.path.join(do_raw_path, f\"merged_{lake_id}.csv\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "        use_do_data = pd.read_csv(lake_data_path, usecols=NORM_COLUMNS)\n",
    "        all_data = pd.concat([all_data, use_do_data], ignore_index=True)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File for lake {lake_id} not found.\")\n",
    "        continue\n",
    "\n",
    "average_mean_feats = all_data.mean().to_dict()\n",
    "average_std_feats = all_data.std().to_dict()\n",
    "\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "np.save(os.path.join(save_path, 'mean_feats.npy'), average_mean_feats)\n",
    "np.save(os.path.join(save_path, 'std_feats.npy'), average_std_feats)\n",
    "\n",
    "loaded_mean_feats = np.load(os.path.join(save_path, 'mean_feats.npy'), allow_pickle=True).item()\n",
    "loaded_std_feats = np.load(os.path.join(save_path, 'std_feats.npy'), allow_pickle=True).item()\n",
    "\n",
    "print(\"Loaded Mean Features:\", loaded_mean_feats)\n",
    "print(\"Loaded Std Features:\", loaded_std_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_raw_path = \"../data/raw/lakes_temp/\"\n",
    "processed_save_path = '../data/processedByLake/'\n",
    "\n",
    "\n",
    "for lake_id in tqdm(ids):\n",
    "    do_data = pd.read_csv(os.path.join(do_raw_path, f\"merged_{lake_id}.csv\"))\n",
    "    norm_do_data = do_data.copy()\n",
    "\n",
    "    for feature in NORM_COLUMNS:\n",
    "        mean_value = loaded_mean_feats[feature]\n",
    "        std_value = loaded_std_feats[feature]\n",
    "        norm_do_data[feature] = norm_do_data[feature].apply(lambda x: (x - mean_value) / std_value if pd.notna(x) and std_value != 0 else x)\n",
    "    \n",
    "    # NaN ----> 0\n",
    "    norm_do_data[NORM_COLUMNS] = norm_do_data[NORM_COLUMNS].fillna(0)\n",
    "\n",
    "    temp_data = pd.read_csv(os.path.join(temp_raw_path, f\"temperature_{lake_id}.csv\"))\n",
    "\n",
    "    for dataset in ['train', 'val', 'test']:\n",
    "        start_date = start_date_list[dataset]\n",
    "        end_date = end_date_list[dataset]\n",
    "\n",
    "        do_data['datetime'] = pd.to_datetime(do_data['datetime']).dt.tz_localize(None)\n",
    "        norm_do_data['datetime'] = pd.to_datetime(norm_do_data['datetime']).dt.tz_localize(None)\n",
    "        temp_data['datetime'] = pd.to_datetime(temp_data['datetime']).dt.tz_localize(None)\n",
    "        \n",
    "        train_raw_do_data = do_data[do_data['datetime'].dt.year <= 2011]\n",
    "        val_raw_do_data = do_data[(do_data['datetime'].dt.year >= 2012) & (do_data['datetime'].dt.year <= 2015)]\n",
    "        test_raw_do_data = do_data[(do_data['datetime'].dt.year >= 2016) & (do_data['datetime'].dt.year <= 2019)]\n",
    "\n",
    "        train_norm_do_data = norm_do_data[norm_do_data['datetime'].dt.year <= 2011]\n",
    "        val_norm_do_data = norm_do_data[(norm_do_data['datetime'].dt.year >= 2012) & (norm_do_data['datetime'].dt.year <= 2015)]\n",
    "        test_norm_do_data = norm_do_data[(norm_do_data['datetime'].dt.year >= 2016) & (norm_do_data['datetime'].dt.year <= 2019)]\n",
    "\n",
    "\n",
    "        train_temp_data = temp_data[temp_data['datetime'].dt.year <= 2011]\n",
    "        val_temp_data = temp_data[(temp_data['datetime'].dt.year >= 2012) & (temp_data['datetime'].dt.year <= 2015)]\n",
    "        test_temp_data = temp_data[(temp_data['datetime'].dt.year >= 2016) & (temp_data['datetime'].dt.year <= 2019)]\n",
    "\n",
    "        save_path = f'../data/processedByLake/{lake_id}/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        train_raw_do_data.to_csv(os.path.join(save_path, 'train_raw_do_data.csv'), index=False)\n",
    "        val_raw_do_data.to_csv(os.path.join(save_path, 'val_raw_do_data.csv'), index=False)\n",
    "        test_raw_do_data.to_csv(os.path.join(save_path, 'test_raw_do_data.csv'), index=False)\n",
    "\n",
    "        train_norm_do_data.to_csv(os.path.join(save_path, 'train_norm_do_data.csv'), index=False)\n",
    "        val_norm_do_data.to_csv(os.path.join(save_path, 'val_norm_do_data.csv'), index=False)\n",
    "        test_norm_do_data.to_csv(os.path.join(save_path, 'test_norm_do_data.csv'), index=False)\n",
    "\n",
    "        train_temp_data.to_csv(os.path.join(save_path, 'train_temp_data.csv'), index=False)\n",
    "        val_temp_data.to_csv(os.path.join(save_path, 'val_temp_data.csv'), index=False)\n",
    "        test_temp_data.to_csv(os.path.join(save_path, 'test_temp_data.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "base_path = \"../data/raw/temperature/\"\n",
    "cfg_path = base_path+\"cfg/pb0_config.json\"\n",
    "obs_df = pd.read_csv(base_path+\"obs/temperature_observations.csv\")\n",
    "# ids = np.unique(obs_df['site_id'].values)\n",
    "\n",
    "cfg_f = open(cfg_path)\n",
    "cfg = json.load(cfg_f)\n",
    "\n",
    "ct = 0\n",
    "\n",
    "for nid in ids: #for each new additional lake\n",
    "    ct += 1\n",
    "    print(\"processing lake \",ct,\": \",nid)\n",
    "    nid = str(nid)\n",
    "    csv = []\n",
    "    csv.append(\"depths,areas\")\n",
    "    m = re.search('nhdhr_(.*)', nid) \n",
    "    name = m.group(1)\n",
    "\n",
    "    hs = np.array(cfg[nid]['morphometry']['H'])\n",
    "    As = cfg[nid]['morphometry']['A']\n",
    "\n",
    "    #get into posi depth form\n",
    "    hs = -(np.flip(hs,axis=0) - hs.max())\n",
    "    As = np.flip(As,axis=0)\n",
    "    assert hs.shape[0] == As.shape[0]\n",
    "\n",
    "    #back to str form\n",
    "    hs = [str(i) for i in hs]\n",
    "    As = [str(i) for i in As]\n",
    "\n",
    "    for i in range(len(hs)):\n",
    "        csv.append(\",\".join([hs[i], As[i]]))\n",
    "\n",
    "    \n",
    "    create_path(f\"processedByLake/{nid}/\")\n",
    "\n",
    "\n",
    "    with open(\"processedByLake/\"+nid+\"/geometry\",'w') as file:\n",
    "        for line in csv:\n",
    "            file.write(line)\n",
    "            file.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
