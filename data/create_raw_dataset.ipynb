{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "\n",
    "import sys\n",
    "ids_pd = pd.read_csv(\"../utils/intersection_ids.csv\")\n",
    "ids = ids_pd['nhdhr_id'].to_list()\n",
    "\n",
    "base_path = \"../data/raw/temperature/\"\n",
    "path_extensions = [\"ice_flags\",\"meteo\",\"predictions\",\"cfg\", \"obs\"]\n",
    "\n",
    "\n",
    "raw_path = '../data/raw/dissolved_oxygen/old_lake_data_all.csv'\n",
    "raw_data = pd.read_csv(raw_path)\n",
    "raw_data = raw_data.drop_duplicates(subset=['nhdhr_id', 'datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_naive(df, col_name):\n",
    "    \"\"\"Ensure the datetime column is timezone naive.\"\"\"\n",
    "    df[col_name] = pd.to_datetime(df[col_name]).dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def expand_with_depths(df, depth_values, date_col):\n",
    "    \"\"\"Expand daily data to include depth information.\"\"\"\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.set_index(date_col)\n",
    "    all_dates = pd.date_range(start_date, end_date)\n",
    "    df = df.reindex(all_dates).ffill().reset_index()\n",
    "    df.rename(columns={'index': date_col}, inplace=True)\n",
    "    expanded = []\n",
    "    for depth in depth_values:\n",
    "        temp_df = df.copy()\n",
    "        temp_df['vertical_depth'] = depth\n",
    "        expanded.append(temp_df)\n",
    "    expanded_df = pd.concat(expanded).reset_index(drop=True)\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_do_features(feature_df):\n",
    "# Delete first day of 1979\n",
    "    # feature_df = feature_df.iloc[1:,:].copy()\n",
    "    feature_df = feature_df\n",
    "    # # Morphometric_list    \n",
    "    # Morphometric_list = ['Shore_len', 'Vol_total', 'Vol_res', 'Vol_src','Depth_avg', \n",
    "    # 'Dis_avg', 'Res_time', 'Elevation', 'Slope_100','Wshd_area']\n",
    "    # for feature in Morphometric_list:\n",
    "    #     missing_count = feature_df[feature].isna().sum()\n",
    "    #     if feature_df[feature].isna().sum() == len(feature_df[feature]):\n",
    "    #         mean_value = mean_std.loc[mean_std['feature'] == feature, 'mean'].values[0]\n",
    "    #         # std_value = mean_std.loc[mean_std['feature'] == feature, 'std'].values[0]\n",
    "    #         # noise = np.random.normal(loc=mean_value, scale=std_value, size=missing_count)\n",
    "    #         feature_df.loc[feature_df[feature].isna(), feature] = mean_value\n",
    "    #         # To-DO \n",
    "\n",
    "    #     elif missing_count > 0:\n",
    "    #         feature_df[feature].fillna(feature_df[feature].mean(), inplace=True)\n",
    "\n",
    "    # Flux data\n",
    "    flux1 = ['fnep', 'fmineral', 'fsed', 'fatm', 'fdiff']\n",
    "    for flux in flux1:\n",
    "        feature_df[flux].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    flux2 = ['fentr_epi', 'fentr_hyp']\n",
    "    for flux in flux2:\n",
    "        feature_df[flux].fillna(0, inplace=True)\n",
    "    \n",
    "    # #Land use\n",
    "    # lanuse_list = ['water', 'developed',\n",
    "    # 'barren', 'forest', 'shrubland', 'herbaceous', 'cultivated', 'wetlands']\n",
    "    # for feature in lanuse_list:\n",
    "    #     missing_count = feature_df[feature].isna().sum()\n",
    "    #     if feature_df[feature].isna().sum() == len(feature_df[feature]):\n",
    "    #         mean_value = mean_std.loc[mean_std['feature'] == feature, 'mean'].values[0]\n",
    "    #         # std_value = mean_std.loc[mean_std['feature'] == feature, 'std'].values[0]\n",
    "    #         # noise = np.random.normal(loc=mean_value, scale=std_value, size=missing_count)\n",
    "    #         feature_df.loc[feature_df[feature].isna(), feature] = mean_value\n",
    "    #         # To-DO \n",
    "\n",
    "    #     elif missing_count > 0:\n",
    "    #         feature_df[feature].fillna(feature_df[feature].mean(), inplace=True)\n",
    "\n",
    "    # Trophic State and Classification\n",
    "    # noise_list = ['eutro', 'oligo', 'dys', 'sat_hypo', 'thermocline_depth']\n",
    "    # for feature in noise_list:\n",
    "    #     mean_value = feature_df[feature].mean()\n",
    "    #     std_value = feature_df[feature].std()\n",
    "    #     missing_count = feature_df[feature].isna().sum()\n",
    "\n",
    "    #     if feature_df[feature].isna().sum() == len(feature_df[feature]):\n",
    "    #         mean_value = mean_std.loc[mean_std['feature'] == feature, 'mean'].values[0]\n",
    "    #         std_value = mean_std.loc[mean_std['feature'] == feature, 'std'].values[0]\n",
    "    #         noise = np.random.normal(loc=mean_value, scale=std_value, size=missing_count)\n",
    "    #         feature_df.loc[feature_df[feature].isna(), feature] = noise\n",
    "\n",
    "    #     elif missing_count > 0:\n",
    "    #         noise = np.random.normal(loc=mean_value, scale=std_value, size=missing_count)\n",
    "    #         feature_df.loc[feature_df[feature].isna(), feature] = noise\n",
    "\n",
    "    # temperature_list = ['temperature_epi', 'temperature_hypo']\n",
    "    # # compare temperature_epi temperature_hypo nan\n",
    "    # mask1 = feature_df[temperature_list[0]].isna()\n",
    "    # mask2 = feature_df[temperature_list[1]].isna()\n",
    "    # if not mask1.equals(mask2):\n",
    "    #     raise Exception(\"temperature_epi and temperature_hypo not match\")\n",
    "    \n",
    "    # assert not feature_df['temperature_total'].isna().any(), \"temperature_total column contains NaN values\"\n",
    "    # # 用 temperature_total 替换 temperature_epi 和 temperature_hypo 中的 NaN 值\n",
    "    # for feature in temperature_list:\n",
    "    #     feature_df[feature] = feature_df[feature].fillna(feature_df['temperature_total'])\n",
    "\n",
    "    # assert not feature_df['temperature_epi'].isna().any(), \"temperature_epi column contains NaN values\"\n",
    "\n",
    "    valid_row = feature_df.dropna(subset=['volume_epi', 'volume_hypo']).iloc[0]\n",
    "\n",
    "    volume_total = valid_row['volume_epi'] + valid_row['volume_hypo']\n",
    "\n",
    "    \n",
    "    feature_df['volume_epi'].fillna(volume_total, inplace=True)\n",
    "    feature_df['volume_hypo'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    # put obs_total in obs_epi\n",
    "    overlap_condition = (~feature_df['obs_tot'].isna()) & (~feature_df['obs_epi'].isna())\n",
    "\n",
    "    if overlap_condition.any():\n",
    "        raise ValueError(\"Error: Overlap detected between 'obs_tot' and 'obs_epi'.\")\n",
    "\n",
    "    feature_df.loc[~feature_df['obs_tot'].isna(), 'obs_epi'] = feature_df['obs_tot']\n",
    " \n",
    "    # when mixed, set sim_hypo = nan\n",
    "    # feature_df.loc[mixed_condition, 'sim_hyp'] = np.nan\n",
    "\n",
    "    # for feature in USE_FEATURES_COLUMNS:\n",
    "    #     all_not_na = feature_df[feature].notna().all()\n",
    "    #     if not all_not_na.all():\n",
    "    #         raise Exception(f\"Data missing in {feature}\")\n",
    "        \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from utils.utils import DROP_COLUMNS\n",
    "\n",
    "# base_path, lake_id, dates_set, ids\n",
    "\n",
    "start_date = pd.to_datetime('1980-01-01').tz_localize(None)\n",
    "end_date = pd.to_datetime('2019-12-31').tz_localize(None)\n",
    "raw_grouped = raw_data.groupby('nhdhr_id')\n",
    "\n",
    "\n",
    "for lake_id, do_data_pd in tqdm(raw_grouped):\n",
    "    do_data_pd = ensure_naive(do_data_pd, 'datetime')\n",
    "    do_data_pd = do_data_pd.drop(columns=DROP_COLUMNS)\n",
    "    # print(\"Filling do data...\")\n",
    "    do_data_pd = fill_do_features(do_data_pd)\n",
    "    do_data_pd = do_data_pd[(do_data_pd['datetime'] >= start_date) & (do_data_pd['datetime'] <= end_date)]\n",
    "    \n",
    "    \n",
    "    ### Ice_flag info\n",
    "    ice_flags_df = pd.read_csv(base_path + 'ice_flags/pb0_' + lake_id + '_ice_flags.csv')\n",
    "    ice_flags_df = ensure_naive(ice_flags_df, 'date')\n",
    "    ice_flags_df = ice_flags_df[(ice_flags_df['date'] >= start_date) & (ice_flags_df['date'] <= end_date)]\n",
    "\n",
    "    ### input features(ice_flag not included)\n",
    "    meteo_df = pd.read_csv(base_path + 'meteo/' + lake_id + '_meteo.csv', delimiter=',', usecols=['time', 'ShortWave', 'LongWave', 'AirTemp', 'RelHum', 'WindSpeed', 'Rain', 'Snow'])\n",
    "    meteo_df = ensure_naive(meteo_df, 'time')\n",
    "    meteo_df = meteo_df[(meteo_df['time'] >= start_date) & (meteo_df['time'] <= end_date)]\n",
    "\n",
    "\n",
    "    do_data_pd['datetime'] = pd.to_datetime(do_data_pd['datetime'])\n",
    "    ice_flags_df['datetime'] = pd.to_datetime(ice_flags_df['date'])\n",
    "    meteo_df['datetime'] = pd.to_datetime(meteo_df['time'])\n",
    "\n",
    "    data_frames = [do_data_pd, ice_flags_df, meteo_df]\n",
    "\n",
    "\n",
    "    row_counts = [df.shape[0] for df in data_frames]\n",
    "\n",
    "\n",
    "    if len(set(row_counts)) != 1:\n",
    "        raise Exception(\"DataFrames do not have the same number of rows\")\n",
    "\n",
    "\n",
    "    all_data = [df.set_index(['datetime']) for df in data_frames]\n",
    "\n",
    "    merged_df = pd.concat(all_data, axis=1).reset_index()\n",
    "\n",
    "    DROP_COLUMNS2 = ['date', 'time']\n",
    "    merged_df = merged_df.drop(columns=DROP_COLUMNS2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(merged_df.head())\n",
    "    # print(\"Result shape:\", merged_df.shape)\n",
    "\n",
    "    create_path(\"../data/raw/lakes/\")\n",
    "    merged_df.to_csv(f'../data/raw/lakes/merged_{lake_id}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from utils.utils import DROP_COLUMNS\n",
    "\n",
    "# base_path, lake_id, dates_set, ids\n",
    "\n",
    "start_date = pd.to_datetime('1980-01-01').tz_localize(None)\n",
    "end_date = pd.to_datetime('2019-12-31').tz_localize(None)\n",
    "raw_grouped = raw_data.groupby('nhdhr_id')\n",
    "\n",
    "\n",
    "for lake_id, do_data_pd in tqdm(raw_grouped):\n",
    "    do_data_pd = ensure_naive(do_data_pd, 'datetime')\n",
    "    do_data_pd = do_data_pd.drop(columns=DROP_COLUMNS)\n",
    "    # print(\"Filling do data...\")\n",
    "    do_data_pd = fill_do_features(do_data_pd)\n",
    "    do_data_pd = do_data_pd[(do_data_pd['datetime'] >= start_date) & (do_data_pd['datetime'] <= end_date)]\n",
    "    \n",
    "    \n",
    "    ### Ice_flag info\n",
    "    ice_flags_df = pd.read_csv(base_path + 'ice_flags/pb0_' + lake_id + '_ice_flags.csv')\n",
    "    ice_flags_df = ensure_naive(ice_flags_df, 'date')\n",
    "    ice_flags_df = ice_flags_df[(ice_flags_df['date'] >= start_date) & (ice_flags_df['date'] <= end_date)]\n",
    "\n",
    "    ### input features(ice_flag not included)\n",
    "    meteo_df = pd.read_csv(base_path + 'meteo/' + lake_id + '_meteo.csv', delimiter=',', usecols=['time', 'ShortWave', 'LongWave', 'AirTemp', 'RelHum', 'WindSpeed', 'Rain', 'Snow'])\n",
    "    meteo_df = ensure_naive(meteo_df, 'time')\n",
    "    meteo_df = meteo_df[(meteo_df['time'] >= start_date) & (meteo_df['time'] <= end_date)]\n",
    "\n",
    "    # create temp_df (features, nhdhr_id, datetime, depth, glm_temp, obs_temp, 7 meteo('ShortWave', 'LongWave', 'AirTemp', 'RelHum', 'WindSpeed', 'Rain', 'Snow'))\n",
    "    # print(\"starting: \", lake_id)\n",
    "\n",
    "    # GLM\n",
    "    glm_df = pd.read_csv(base_path + 'predictions/pb0_' + lake_id + '_temperatures.csv')\n",
    "    glm_df = ensure_naive(glm_df, 'date')\n",
    "    glm_df = glm_df[(glm_df['date'] >= start_date) & (glm_df['date'] <= end_date)]\n",
    "    n_depths = glm_df.shape[1] - 1  # 减去日期列\n",
    "    max_depth = 0.5 * (n_depths - 1)\n",
    "    depths = np.arange(0, max_depth + 0.5, 0.5)\n",
    "\n",
    "    # 初始化sim_save数组，形状为(n_depths, n_dates)\n",
    "    sim_save = glm_df.iloc[:, 1:].values.T\n",
    "\n",
    " \n",
    "    # print(\"Fill nan data in sim temperature...\")\n",
    "    if np.isnan(np.sum(sim_save)):\n",
    "        for i in range(n_depths):\n",
    "            for t in range(sim_save.shape[1]):\n",
    "                if np.isnan(sim_save[i, t]):\n",
    "                    x = depths[i]\n",
    "                    xp = depths[0:(i)]\n",
    "                    yp = sim_save[0:(i), t]\n",
    "                    if xp.shape[0] == 1:\n",
    "                        sim_save[i, t] = sim_save[i - 1, t]\n",
    "                    else:\n",
    "                        f = interpolate.interp1d(xp, yp, fill_value=\"extrapolate\")\n",
    "                        sim_save[i, t] = f(x)\n",
    "\n",
    "\n",
    "    obs_df = pd.read_csv(base_path + 'obs/' + lake_id + '_obs.csv', usecols=['date', 'depth', 'temp'])\n",
    "    obs_df = obs_df.rename(columns={'depth': 'vertical_depth'})\n",
    "    obs_df = ensure_naive(obs_df, 'date')\n",
    "    obs_df = obs_df[(obs_df['date'] >= start_date) & (obs_df['date'] <= end_date)]\n",
    "    obs_df = obs_df[obs_df['vertical_depth'] <= max_depth]\n",
    "    obs_df.sort_values(by='date', inplace=True)\n",
    "\n",
    "\n",
    "    depth_indices = {depth: idx for idx, depth in enumerate(depths)}\n",
    "    dates_indices = pd.Series(index=pd.to_datetime(glm_df.iloc[:, 0]), data=np.arange(len(glm_df)))\n",
    "\n",
    "\n",
    "    # print(\"Processing glm temperature data...\")\n",
    "    temperature = []\n",
    "\n",
    "\n",
    "    dates = pd.to_datetime(glm_df.iloc[:, 0])\n",
    "    date_depth_pairs = [(d, depth) for d in dates for depth in depths]\n",
    "    temperature_df = pd.DataFrame(date_depth_pairs, columns=['datetime', 'vertical_depth'])\n",
    "    temperature_df['lake_id'] = lake_id\n",
    "    temperature_df = pd.DataFrame(date_depth_pairs, columns=['datetime', 'vertical_depth'])\n",
    "\n",
    "    reshaped_sim_save = sim_save.T.flatten()\n",
    "    temperature_df['sim_temp'] = reshaped_sim_save\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"Processing obs temperature data...\")\n",
    "    temperature_df = temperature_df.merge(obs_df, left_on=['datetime', 'vertical_depth'], right_on=['date', 'vertical_depth'], how='left').drop(columns='date')\n",
    "    temperature_df.rename(columns={'temp': 'obs_temp'}, inplace=True)\n",
    "\n",
    "\n",
    "    # print(\"expand do data...\")\n",
    "    do_data_expanded = expand_with_depths(do_data_pd, depths, 'datetime')\n",
    "    # print(\"expand ice_flag data...\")\n",
    "    ice_flags_expanded = expand_with_depths(ice_flags_df, depths, 'date')\n",
    "    # print(\"expand meteo data...\")\n",
    "    meteo_expanded = expand_with_depths(meteo_df, depths, 'time')\n",
    "    # print(\"do_data_expanded:\", do_data_expanded)\n",
    "\n",
    "    do_data_expanded['datetime'] = pd.to_datetime(do_data_expanded['datetime'])\n",
    "    ice_flags_expanded['datetime'] = pd.to_datetime(ice_flags_expanded['date'])\n",
    "    meteo_expanded['datetime'] = pd.to_datetime(meteo_expanded['time'])\n",
    "\n",
    "    data_frames = [temperature_df, do_data_expanded, ice_flags_expanded, meteo_expanded]\n",
    "\n",
    "\n",
    "    row_counts = [df.shape[0] for df in data_frames]\n",
    "\n",
    "\n",
    "    if len(set(row_counts)) != 1:\n",
    "        raise Exception(\"DataFrames do not have the same number of rows\")\n",
    "\n",
    "\n",
    "    all_data = [df.set_index(['datetime', 'vertical_depth']) for df in data_frames]\n",
    "\n",
    "    merged_df = pd.concat(all_data, axis=1).reset_index()\n",
    "\n",
    "    # Adding layer feature\n",
    "\n",
    "    merged_df['layer'] = np.where(merged_df['vertical_depth'] < merged_df['thermocline_depth'], 0, 1)\n",
    "    \n",
    "    DROP_COLUMNS2 = ['date', 'time']\n",
    "    merged_df = merged_df.drop(columns=DROP_COLUMNS2)\n",
    "\n",
    "    # print(merged_df.head())\n",
    "    # print(\"Result shape:\", merged_df.shape)\n",
    "\n",
    "    merged_df.to_csv(f'../data/raw/lakes/merged_{lake_id}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "base_path = \"../data/raw/temperature/\"\n",
    "cfg_path = base_path+\"cfg/pb0_config.json\"\n",
    "obs_df = pd.read_csv(base_path+\"obs/temperature_observations.csv\")\n",
    "ids = np.unique(obs_df['site_id'].values)\n",
    "\n",
    "cfg_f = open(cfg_path)\n",
    "cfg = json.load(cfg_f)\n",
    "\n",
    "ct = 0\n",
    "\n",
    "for nid in ids: #for each new additional lake\n",
    "    ct += 1\n",
    "    print(\"processing lake \",ct,\": \",nid)\n",
    "    nid = str(nid)\n",
    "    csv = []\n",
    "    csv.append(\"depths,areas\")\n",
    "    m = re.search('nhdhr_(.*)', nid) \n",
    "    name = m.group(1)\n",
    "\n",
    "    hs = np.array(cfg[nid]['morphometry']['H'])\n",
    "    As = cfg[nid]['morphometry']['A']\n",
    "\n",
    "    #get into posi depth form\n",
    "    hs = -(np.flip(hs,axis=0) - hs.max())\n",
    "    As = np.flip(As,axis=0)\n",
    "    assert hs.shape[0] == As.shape[0]\n",
    "\n",
    "    #back to str form\n",
    "    hs = [str(i) for i in hs]\n",
    "    As = [str(i) for i in As]\n",
    "\n",
    "    for i in range(len(hs)):\n",
    "        csv.append(\",\".join([hs[i], As[i]]))\n",
    "\n",
    "    \n",
    "    create_path(f\"processedByLake/{nid}/\")\n",
    "\n",
    "\n",
    "    with open(\"processedByLake/\"+nid+\"/geometry\",'w') as file:\n",
    "        for line in csv:\n",
    "            file.write(line)\n",
    "            file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "for dataset in ['train', 'test', 'val']:\n",
    "    bucket_do_pd = pd.read_csv(f\"processed/processed_{dataset}_do_processed.0.csv\")\n",
    "    bucket_temp_pd = pd.read_csv(f\"processed/processed_{dataset}_temp_processed.0.csv\")\n",
    "\n",
    "    for lake_id, group in bucket_do_pd.groupby('nhdhr_id'):\n",
    "        output_dir = f\"processedByLake/{lake_id}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_file = f\"{output_dir}/bucket_do_{lake_id}_{dataset}.csv\"\n",
    "        group.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Saved {output_file}\")\n",
    "\n",
    "    for lake_id, group in bucket_temp_pd.groupby('nhdhr_id'):\n",
    "        output_dir = f\"processedByLake/{lake_id}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_file = f\"{output_dir}/bucket_temp_{lake_id}_{dataset}.csv\"\n",
    "        group.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Saved {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
